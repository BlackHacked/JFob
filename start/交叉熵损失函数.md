---
tags: 名词解释
alias: cross-entropy loss function
---

交叉熵损失函数（cross-entropy loss function）
?
是深度学习中常用的一种损失函数，通常用于分类问题。它通过比较预测概率分布与真实标签的概率分布之间的差异来评估模型的预测效果。具体来说，对于一个具有 $K$ 个类别的分类问题，在模型输出的 $K$ 维向量中，真实标签对应的元素值为 1，其余元素值为 0。模型输出的向量经过 softmax 函数后得到各个类别的概率分布，交叉熵损失则是将该概率分布与真实标签的概率分布进行比较，计算它们之间的交叉熵值，用于衡量模型的预测与真实标签之间的差距。交叉熵损失函数的值越小，表示模型的预测效果越好。
交叉熵（cross-entropy）是一种常用的损失函数，常用于分类问题的模型训练中。计算交叉熵的公式如下：
$$H(p,q)=-\sum_{i=1}^{n}p_i\log(q_i)$$
其中，$p$ 是样本的真实标签分布，$q$ 是模型输出的标签分布，$n$ 是类别的数量。交叉熵越小，则说明模型输出的标签分布越接近真实标签分布，模型的分类性能越好。
在深度学习模型的训练中，通常将交叉熵作为损失函数，使用反向传播算法来更新模型参数，以最小化交叉熵损失。
<!--SR:!2023-05-19,1,230-->